# üè† House Price Prediction

> Predicting residential home sale prices in Ames, Iowa using machine learning regression models.

![Python](https://img.shields.io/badge/Python-‚â•3.14-3776AB?logo=python&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-‚â•1.8.0-F7931E?logo=scikit-learn&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-‚â•3.0.1-150458?logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-‚â•2.4.2-013243?logo=numpy&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-‚â•3.10.8-11557C)

---

## üìå Overview

This project tackles a classic regression problem ‚Äî **predicting house sale prices** ‚Äî using the **Ames Housing dataset**. It covers the full ML workflow: data cleaning, exploratory analysis, preprocessing pipelines, and model comparison.

The codebase is split into two clear purposes:

| | File | Purpose |
|---|---|---|
| üìì | `notebooks/exploration.ipynb` | Interactive EDA & experimentation ‚Äî plots inline, markdown storytelling |
| üè≠ | `src/train.py` | Reproducible training script ‚Äî no `plt.show()`, saves models & figures via CLI |

---

## üìÇ Project Structure

```
house-price-prediction/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ ames-housing.csv               # 2,930 samples √ó 82 features
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ data-dictionary.md             # Full feature reference (formatted)
‚îú‚îÄ‚îÄ models/                            # Serialized sklearn pipelines (.pkl)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ exploration.ipynb              # Interactive EDA notebook
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îî‚îÄ‚îÄ figures/                       # Saved plots (.pdf)
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ train.py                       # Training script (CLI)
```

> `models/` and `reports/figures/` are auto-generated by `train.py`.

---

## üìä Dataset

**Ames Housing dataset** ‚Äî 2,930 residential property sales from Ames, Iowa (2006‚Äì2010) with 82 features describing nearly every aspect of a home.

üì© Download from [Kaggle](https://www.kaggle.com/datasets/prevek18/ames-housing-dataset)

| Category | Examples |
|----------|----------|
| üèóÔ∏è **Structure** | Building type, house style, year built, overall quality |
| üìê **Size** | Lot area, living area, basement SF, garage area |
| üõèÔ∏è **Interior** | Bedrooms, bathrooms, kitchen quality, total rooms |
| üå≥ **Exterior** | Roof style/material, siding, porch/deck area |
| üìç **Location** | Neighborhood, zoning, lot configuration |
| üí∞ **Sale** | Sale type, condition, **Sale Price (target)** |

üìñ See [docs/data-dictionary.md](docs/data-dictionary.md) for the full feature reference.

---

## üîÑ Workflow

### 1. Data Cleaning üßπ
- Standardized column names (stripped whitespace and spaces)
- Removed irrelevant identifiers (`Order`, `PID`)
- Cleaned whitespace from categorical values

### 2. Exploratory Data Analysis üìà
- Target distribution (right-skewed ‚Üí motivates log transform)
- Missing values audit across all features
- Pearson correlation of numeric features with `SalePrice`
- Distribution plots for all numeric and categorical features

### 3. Preprocessing ‚öôÔ∏è
- **Numeric features** ‚Üí median imputation
- **Categorical features** ‚Üí most-frequent imputation ‚Üí One-Hot Encoding
- **Target** ‚Üí log transformation (`log1p`) to reduce skewness
- Unified `ColumnTransformer` pipeline (no data leakage)

### 4. Model Training ü§ñ
Two models trained with **5-fold cross-validation** (R¬≤ scoring):

| Model | Target | Configuration |
|-------|--------|---------------|
| Linear Regression | Raw `SalePrice` | Baseline |
| Random Forest | Log-transformed `SalePrice` | 200 estimators, `n_jobs=-1` |

---

## üìà Results

| Metric | Linear Regression | Random Forest |
|--------|:-----------------:|:-------------:|
| **Mean R¬≤** | 0.8592 | **0.8815** ‚≠ê |
| **Std Dev** | ¬±0.0507 | ¬±0.0149 |
| Min R¬≤ | 0.7814 | 0.8684 |
| Max R¬≤ | 0.9090 | 0.9028 |

### üí° Key Takeaways

- üå≤ **Random Forest outperforms Linear Regression** ‚Äî handles non-linear relationships and high-cardinality categoricals better
- üìâ **Log-transforming the target** stabilized training and reduced variance across folds
- üîÑ **Cross-validation** shows Random Forest is more consistent (lower std dev) with fewer performance dips
- üîó **Pipeline approach** keeps preprocessing + model tightly coupled ‚Äî prevents data leakage

---

## üõ†Ô∏è Tech Stack

| Tool | Version | Purpose |
|------|---------|---------|
| **pandas** | ‚â•3.0.1 | Data loading & manipulation |
| **NumPy** | ‚â•2.4.2 | Numerical operations & log transform |
| **Matplotlib** | ‚â•3.10.8 | Data visualization & figure export |
| **scikit-learn** | ‚â•1.8.0 | Preprocessing, pipelines & models |
| **joblib** | *(bundled)* | Model serialization |

---

## üöÄ Getting Started

> This project is part of the [`data-science-projects`](https://github.com/DanciVasile/data-science-projects) repository. Dependencies are managed at the root level with [**uv**](https://docs.astral.sh/uv/).

```bash
# Clone & install
git clone https://github.com/DanciVasile/data-science-projects.git
cd data-science-projects
uv sync

# Train models
cd house-price-prediction
python src/train.py
```

The script saves trained models to `models/` and publication-quality plots to `reports/figures/`.

For interactive exploration, open the notebook:

```bash
jupyter notebook notebooks/exploration.ipynb
```

---

<p align="center">
  Made with ‚ù§Ô∏è as part of my Data Science portfolio
  <br/>
  <a href="https://github.com/DanciVasile">GitHub</a> ¬∑ 
  <a href="https://www.linkedin.com/in/vasile-danci-m/">LinkedIn</a>
</p>
