# ğŸ  House Price Prediction

> Predicting residential home sale prices in Ames, Iowa using machine learning regression models.

![Python](https://img.shields.io/badge/Python-3.14-blue?logo=python&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.8-orange?logo=scikit-learn&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-3.0-purple?logo=pandas&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-3.10-green?logo=plotly&logoColor=white)

---

## ğŸ“Œ Overview

This project tackles a classic regression problem â€” **predicting house sale prices** â€” using the well-known **Ames Housing dataset**. It walks through the full data science workflow: from data cleaning and exploratory analysis to building, evaluating, and comparing machine learning models.

## ğŸ“‚ Project Structure

```
house-price-prediction/
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ames-housing.csv               # Ames Housing dataset (~2,930 samples, 82 features)
â”‚   â””â”€â”€ data-categories.txt            # Raw feature category definitions
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ data-dictionary.md             # Formatted data dictionary (Markdown)
â”œâ”€â”€ models/                            # Trained model artifacts (.pkl) â€” generated by train.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb              # Interactive EDA & experimentation notebook
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ figures/                       # Saved plots (.pdf) â€” generated by train.py
â””â”€â”€ src/
    â””â”€â”€ train.py                       # Reproducible training script (CLI)
```

| Directory | Purpose |
|-----------|---------|
| `notebooks/` | ğŸ““ **Exploration & storytelling** â€” interactive EDA with inline plots and markdown |
| `src/` | ğŸ­ **Production-ready training** â€” clean script, no `plt.show()`, saves models & figures |
| `models/` | ğŸ’¾ Serialized sklearn pipelines (`.pkl`) â€” auto-generated, gitignored |
| `reports/figures/` | ğŸ“Š Publication-quality plots (`.pdf`) â€” auto-generated by `train.py` |
| `docs/` | ğŸ“– Data dictionary and documentation |

## ğŸ“Š Dataset

The **Ames Housing dataset** contains **2,930 residential property sales** from Ames, Iowa, with **82 features** describing nearly every aspect of a home:

| Feature Category | Examples |
|---|---|
| ğŸ—ï¸ Structure | Building type, house style, year built, overall quality |
| ğŸ“ Size | Lot area, living area, basement SF, garage area |
| ğŸ›ï¸ Rooms | Bedrooms, bathrooms, kitchen quality, total rooms |
| ğŸŒ³ Exterior | Roof style, exterior material, porch/deck area, pool |
| ğŸ“ Location | Neighborhood, zoning, lot configuration |
| ğŸ’° Sale Info | Sale type, sale condition, **Sale Price (target)** |

## ğŸ” Workflow

### 1. Data Cleaning ğŸ§¹
- Standardized column names (stripped whitespace, removed spaces)
- Removed irrelevant columns (`Order`, `PID`)
- Cleaned whitespace from categorical feature values

### 2. Exploratory Data Analysis ğŸ“ˆ
- **Target distribution** â€” visualized `SalePrice` distribution (right-skewed)
- **Missing values audit** â€” identified features with missing data
- **Categorical analysis** â€” counted unique values and plotted distributions
- **Correlation analysis** â€” computed Pearson correlation of all numeric features with `SalePrice`
- **Feature histograms** â€” plotted distributions for all numeric and categorical features

### 3. Feature Engineering âš™ï¸
- **Log transformation** on the target variable (`log1p`) to reduce skewness and stabilize model training
- **Numeric imputation** â€” filled missing values using the **median** strategy
- **Categorical encoding** â€” applied **most-frequent imputation** followed by **One-Hot Encoding**
- Built a unified `ColumnTransformer` preprocessing pipeline

### 4. Modeling ğŸ¤–
Two regression models were trained and evaluated using **5-fold cross-validation** with RÂ² scoring:

| Model | Target | Strategy |
|---|---|---|
| **Linear Regression** | Raw `SalePrice` | Baseline model |
| **Random Forest Regressor** | Log-transformed `SalePrice` | 200 estimators, parallelized (`n_jobs=-1`) |

### 5. Evaluation & Results ğŸ“‰
- Cross-validated **RÂ² scores** were compared across both models
- Predictions from the log-transformed model were **inverse-transformed** (`expm1`) back to the original dollar scale

## âœ… Key Takeaways

- ğŸŒ² **Random Forest > Linear Regression** for this dataset â€” it better handles non-linear relationships and high-cardinality categorical features
- ğŸ“‰ **Log-transforming the target** helped stabilize training and improve consistency across folds
- ğŸ”„ **Cross-validation** confirmed Random Forest delivers more robust, consistent performance with fewer drastic dips

## ğŸ› ï¸ Tech Stack

| Tool | Purpose |
|---|---|
| **pandas** | Data loading & manipulation |
| **NumPy** | Numerical operations & log transform |
| **Matplotlib** | Data visualization |
| **scikit-learn** | Preprocessing, pipelines & models |
| **joblib** | Model serialization |

## ğŸš€ Getting Started

This project is part of the [`data-science-projects`](../../README.md) repository. Dependencies are managed at the root level with **uv**.

```bash
# Clone the repository
git clone https://github.com/DanciVasile/data-science-projects.git
cd data-science-projects

# Install dependencies with uv
uv sync

# Run the training script
cd house-price-prediction
python src/train.py
```

The script will output trained models to `models/` and figures to `reports/figures/`.

For interactive exploration, open `notebooks/exploration.ipynb` in Jupyter or VS Code.

---

<p align="center">
  Made with â¤ï¸ as part of my Data Science portfolio
</p>