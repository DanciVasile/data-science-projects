# ğŸ  House Price Prediction

> Predicting residential home sale prices in Ames, Iowa using machine learning regression models.

![Python](https://img.shields.io/badge/Python-3.14-blue?logo=python&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.8.0-orange?logo=scikit-learn&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-3.0.1-purple?logo=pandas&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-3.10.8-green?logo=matplotlib&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-blue)

---

## ğŸ“Œ Overview

This project tackles a classic regression problem â€” **predicting house sale prices** â€” using the well-known **Ames Housing dataset**. It walks through the full machine learning workflow: from data cleaning and exploratory analysis to building, evaluating, and comparing regression models.

The implementation emphasizes **production-ready code** with clean logging, modular design, and reproducibility.

---

## ğŸ“‚ Project Structure

```
house-price-prediction/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ pyproject.toml                     # uv project configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ames-housing.csv               # Ames Housing dataset (2,930 samples, 82 features)
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ data-dictionary.md             # Feature descriptions & metadata
â”œâ”€â”€ models/                            # Trained model artifacts (.pkl) â€” auto-generated
â”‚   â”œâ”€â”€ linear_regression.pkl
â”‚   â””â”€â”€ random_forest.pkl
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb              # Interactive EDA & experimentation
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ figures/                       # Publication-quality plots (.pdf)
â”‚       â”œâ”€â”€ 01_sale_price_distribution.pdf
â”‚       â”œâ”€â”€ 02_log_sale_price_distribution.pdf
â”‚       â”œâ”€â”€ 03_numeric_features_distribution.pdf
â”‚       â””â”€â”€ 04_categorical_features_distribution.pdf
â””â”€â”€ src/
    â””â”€â”€ train.py                       # Main training script with logging
```

### Directory Guide

| Path | Purpose |
|------|---------|
| `notebooks/` | ğŸ““ Exploration & storytelling with interactive plots |
| `src/train.py` | ğŸ­ Production scriptâ€”clean, logged, reproducible |
| `models/` | ğŸ’¾ Serialized sklearn pipelines (gitignored) |
| `reports/figures/` | ğŸ“Š Auto-generated visualizations as PDFs |
| `docs/` | ğŸ“– Data dictionary & documentation |

---

## ğŸ“Š Dataset Overview

The **Ames Housing dataset** contains **2,930 residential property sales** from Ames, Iowa (2006â€“2010) with **82 features** describing nearly every aspect of a home:

### Feature Categories

| Category | Examples |
|----------|----------|
| ğŸ—ï¸ **Structure** | Building type, house style, year built, overall quality/condition |
| ğŸ“ **Size** | Lot area, living area, basement area, garage area |
| ğŸ›ï¸ **Interior** | Bedrooms, bathrooms, kitchen quality, flooring |
| ğŸŒ³ **Exterior** | Roof style/material, siding, porch/deck area, pool |
| ğŸ“ **Location** | Neighborhood, zoning, lot shape & configuration |
| ğŸ’° **Sale Info** | Sale type, sale condition, **sale price (target)** |

**Target Variable:** `SalePrice` (continuous, right-skewed, range: $34.9Kâ€“$755K)

---

## ğŸ”„ Workflow & Methods

### 1ï¸âƒ£ Data Cleaning ğŸ§¹
- âœ“ Standardized column names (stripped whitespace, removed spaces)
- âœ“ Removed irrelevant columns (`Order`, `PID`)
- âœ“ Cleaned whitespace from categorical values
- âœ“ Analyzed missing values per feature

### 2ï¸âƒ£ Exploratory Data Analysis ğŸ“ˆ
- **Target analysis** â€” visualized `SalePrice` distribution (right-skewed)
- **Missing value audit** â€” identified and logged features with missing data
- **Categorical overview** â€” counted unique values for all categorical features
- **Correlation study** â€” computed Pearson correlation of numeric features with target
- **Feature distributions** â€” generated histograms for all numeric and categorical features

### 3ï¸âƒ£ Preprocessing Pipeline âš™ï¸

**Numeric Features:**
- Strategy: **Median imputation** for missing values
- Rationale: Robust to outliers, preserves distribution

**Categorical Features:**
- Strategy: **Most-frequent imputation** â†’ **One-Hot Encoding**
- Rationale: Handles missing categories, enables linear models to use categorical data

**Target Variable:**
- Applied `log1p()` transformation to reduce right-skewness
- Helps stabilize model training and improve convergence

### 4ï¸âƒ£ Model Training ğŸ¤–

Two regression models trained using **5-fold cross-validation** with RÂ² scoring:

#### Linear Regression (Baseline)
- **Model:** Vanilla linear regression
- **Target:** Raw `SalePrice`
- **Use:** Baseline for comparison
- **Pros:** Interpretable, fast
- **Cons:** Assumes linear relationships

#### Random Forest Regressor (Best)
- **Model:** 200 decision trees, parallelized (`n_jobs=-1`)
- **Target:** Log-transformed `SalePrice`
- **Predictions:** Inverse-transformed back to original scale
- **Pros:** Handles non-linearity, robust to outliers, feature interactions
- **Cons:** Less interpretable, slower inference

### 5ï¸âƒ£ Evaluation & Results ğŸ“Š

```
Linear Regression:  RÂ² = 0.8592 (Â±0.0507)
Random Forest:      RÂ² = 0.8815 (Â±0.0149)  â­ WINNER
```

**Key Findings:**
- ğŸŒ² **Random Forest outperforms** Linear Regression on this dataset
- ğŸ“‰ **Log-transformation** improved stability across folds
- ğŸ¯ Random Forest shows **higher consistency** (lower std dev)
- ğŸ’ª Both models exceed RÂ² = 0.85, indicating strong predictive power

---

## ğŸ“ Logging & Output

The training script uses **Python's logging module** for professional, structured output with emoji indicators:

```python
# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s"
)
logger = logging.getLogger(__name__)
```

### Output Features

- ğŸ¨ **Emoji-enhanced** section headers for visual clarity
- ğŸ“Š **Performance metrics** with auto-generated quality indicators:
  - ğŸ”¥ **Excellent** (RÂ² â‰¥ 0.95)
  - â­ **Great** (RÂ² â‰¥ 0.90)
  - ğŸ‘ **Good** (RÂ² â‰¥ 0.80)
  - ğŸ“Š **Okay** (RÂ² â‰¥ 0.70)
  - âš ï¸ **Poor** (RÂ² < 0.70)
- ğŸ“ **Clear file confirmations** when saving models and figures
- ğŸ† **Automatic model comparison** with winner announcement

### Example Console Output

```
ğŸ  AMES HOUSING PRICE PREDICTION
===========================================================================

===========================================================================
  ğŸ” LOADING DATA
===========================================================================
  âœ“ Loaded 2,930 samples, 79 features
  âœ“ Target variable: SalePrice (mean: $180,921)

===========================================================================
  ğŸ“ˆ SAVING EDA FIGURES
===========================================================================
  ğŸ“Š Saved reports/figures/01_sale_price_distribution.pdf
  ğŸ“Š Saved reports/figures/02_log_sale_price_distribution.pdf
  ğŸ“Š Saved reports/figures/03_numeric_features_distribution.pdf
  ğŸ“Š Saved reports/figures/04_categorical_features_distribution.pdf
  âœ“ Generated 4 exploratory figures

===========================================================================
  ğŸ”— LINEAR REGRESSION
===========================================================================

  ğŸ‘ Linear Regression Performance:
     Mean RÂ²:     0.8592
     Std Dev:     0.0507
     Min RÂ²:      0.7814
     Max RÂ²:      0.9090
     Assessment: Good fit! âœ“

  ğŸ’¾ Saved models/linear_regression.pkl

===========================================================================
  ğŸŒ² RANDOM FOREST (log-transformed target)
===========================================================================

  â­ Random Forest Performance:
     Mean RÂ²:     0.8815
     Std Dev:     0.0149
     Min RÂ²:      0.8684
     Max RÂ²:      0.9028
     Assessment: Great performance! ğŸš€

  ğŸ”® Sample predictions (original scale):
     ['$203,779', '$112,103', '$165,716', '$255,745', '$188,901']

  ğŸ’¾ Saved models/random_forest.pkl

===========================================================================
  ğŸ† MODEL COMPARISON
===========================================================================
  Linear Regression:  0.8592 (Â±0.0507)
  Random Forest:      0.8815 (Â±0.0149)

  ğŸ¯ Winner: Random Forest by 0.0223

===========================================================================
  âœ… TRAINING COMPLETE
===========================================================================
  ğŸ“ Output Locations:
     â€¢ Models:  models/
     â€¢ Figures: reports/figures/
```

---

## ğŸ› ï¸ Tech Stack

| Tool | Version | Purpose |
|------|---------|---------|
| **Python** | 3.10+ | Programming language |
| **pandas** | 2.0+ | Data loading & manipulation |
| **NumPy** | 1.24+ | Numerical operations |
| **scikit-learn** | 1.5+ | ML models & preprocessing |
| **Matplotlib** | 3.8+ | Data visualization |
| **joblib** | 1.3+ | Model serialization |

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- [uv](https://docs.astral.sh/uv/) package manager (recommended)
- Git

### Installation & Running

```bash
# Clone the repository
git clone https://github.com/DanciVasile/data-science-projects.git
cd data-science-projects

# Install dependencies
uv sync

# Navigate to project
cd house-price-prediction

# Run training script
python src/train.py
```

**Output:**
- âœ… Trained models saved to `models/`
- âœ… Visualizations saved to `reports/figures/`
- âœ… Detailed logs printed to console

### Interactive Exploration

```bash
# Open Jupyter notebook for EDA
jupyter notebook notebooks/exploration.ipynb
```

---

## ğŸ“ˆ Results Summary

| Metric | Linear Regression | Random Forest |
|--------|-------------------|---------------|
| **Mean RÂ²** | 0.8592 | 0.8815 â­ |
| **Std Dev** | 0.0507 | 0.0149 |
| **Min RÂ²** | 0.7814 | 0.8684 |
| **Max RÂ²** | 0.9090 | 0.9028 |
| **Consistency** | Good | Excellent |

**Conclusion:** Random Forest is the recommended model for this dataset due to superior performance, robustness, and consistency.

---

## ğŸ’¡ Key Insights

- ğŸŒ² **Ensemble methods** outperform simple linear models on complex, non-linear datasets
- ğŸ“‰ **Target transformation** (log-scaling) improves model stability and generalization
- ğŸ”„ **Cross-validation** is essential for reliable performance estimates
- ğŸ“Š **Professional logging** enables reproducibility and production-ready ML pipelines

---

## ğŸ“š Files & Outputs

### Generated During Training

| File | Purpose |
|------|---------|
| `models/linear_regression.pkl` | Serialized Linear Regression pipeline |
| `models/random_forest.pkl` | Serialized Random Forest pipeline |
| `reports/figures/01_sale_price_distribution.pdf` | Sale price distribution histogram |
| `reports/figures/02_log_sale_price_distribution.pdf` | Log-transformed distribution histogram |
| `reports/figures/03_numeric_features_distribution.pdf` | All numeric features histograms |
| `reports/figures/04_categorical_features_distribution.pdf` | All categorical features bar charts |

---

## ğŸ”— Related

- **Dataset Source:** [Kaggle Ames Housing](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
- **Parent Repository:** [`data-science-projects`](../../README.md)

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see LICENSE file for details.

---

<p align="center">
  Made with â¤ï¸ as part of my Data Science portfolio
  <br/>
  <a href="https://github.com/DanciVasile">GitHub</a> â€¢ 
  <a href="https://www.linkedin.com/in/vasile-danci-m/">LinkedIn</a>
</p>