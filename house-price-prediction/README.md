# ğŸ  House Price Prediction

## ğŸ“Œ Overview

This project tackles a classic regression problem â€” **predicting house sale prices** â€” using the well-known **Ames Housing dataset**. It walks through the full data science workflow: from data cleaning and exploratory analysis to building, evaluating, and comparing machine learning models.

## ğŸ“‚ Project Structure

```
house-price-prediction/
â”œâ”€â”€ main.py                        # Full pipeline (EDA â†’ Preprocessing â†’ Modeling)
â”œâ”€â”€ README.md
â””â”€â”€ dataset/
    â”œâ”€â”€ ames-housing.csv           # Ames Housing dataset (~2,930 observations, 82 features)
    â””â”€â”€ data-categories.txt        # Feature descriptions and category mappings
```

## ğŸ“Š Dataset

The **Ames Housing dataset** contains **2,930 residential property sales** from Ames, Iowa, with **82 features** describing nearly every aspect of a home:

| Feature Category | Examples |
|---|---|
| ğŸ—ï¸ Structure | Building type, house style, year built, overall quality |
| ğŸ“ Size | Lot area, living area, basement SF, garage area |
| ğŸ›ï¸ Rooms | Bedrooms, bathrooms, kitchen quality, total rooms |
| ğŸŒ³ Exterior | Roof style, exterior material, porch/deck area, pool |
| ğŸ“ Location | Neighborhood, zoning, lot configuration |
| ğŸ’° Sale Info | Sale type, sale condition, **Sale Price (target)** |

## ğŸ” Workflow

### 1. Data Cleaning ğŸ§¹
- Standardized column names (stripped whitespace, removed spaces)
- Removed irrelevant columns (`Order`, `PID`)
- Cleaned whitespace from categorical feature values

### 2. Exploratory Data Analysis ğŸ“ˆ
- **Target distribution** â€” visualized `SalePrice` distribution (right-skewed)
- **Missing values audit** â€” identified features with missing data
- **Categorical analysis** â€” counted unique values and plotted distributions
- **Correlation analysis** â€” computed Pearson correlation of all numeric features with `SalePrice`
- **Feature histograms** â€” plotted distributions for all numeric and categorical features

### 3. Feature Engineering âš™ï¸
- **Log transformation** on the target variable (`log1p`) to reduce skewness and stabilize model training
- **Numeric imputation** â€” filled missing values using the **median** strategy
- **Categorical encoding** â€” applied **most-frequent imputation** followed by **One-Hot Encoding**
- Built a unified `ColumnTransformer` preprocessing pipeline

### 4. Modeling ğŸ¤–
Two regression models were trained and evaluated using **5-fold cross-validation** with RÂ² scoring:

| Model | Target | Strategy |
|---|---|---|
| **Linear Regression** | Raw `SalePrice` | Baseline model |
| **Random Forest Regressor** | Log-transformed `SalePrice` | 200 estimators, parallelized (`n_jobs=-1`) |

### 5. Evaluation & Results ğŸ“‰
- Cross-validated **RÂ² scores** were compared across both models
- Predictions from the log-transformed model were **inverse-transformed** (`expm1`) back to the original dollar scale

## âœ… Key Takeaways

- ğŸŒ² **Random Forest > Linear Regression** for this dataset â€” it better handles non-linear relationships and high-cardinality categorical features
- ğŸ“‰ **Log-transforming the target** helped stabilize training and improve consistency across folds
- ğŸ”„ **Cross-validation** confirmed Random Forest delivers more robust, consistent performance with fewer drastic dips

## ğŸ› ï¸ Tech Stack

| Tool | Purpose |
|---|---|
| **pandas** | Data loading & manipulation |
| **NumPy** | Numerical operations & log transform |
| **Matplotlib** | Data visualization |
| **scikit-learn** | Preprocessing, pipelines & models |


