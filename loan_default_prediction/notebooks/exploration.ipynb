{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907c8d22",
   "metadata": {},
   "source": [
    "# üî¨ Loan Default Prediction ‚Äî Exploratory Data Analysis\n",
    "\n",
    "**Author:** Vasile-Marian Danci  \n",
    "**Date:** 2026-03-01  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objective\n",
    "\n",
    "> Describe the goal of this analysis in one or two sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc19e3f",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ 1 ¬∑ Imports\n",
    "\n",
    "Import all required packages here. Keep standard-library, third-party, and local imports separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation ‚Äî swap with plotly / altair / any library you prefer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25029270",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öôÔ∏è 2 ¬∑ Configuration\n",
    "\n",
    "Define all configurable parameters (paths, constants, column names) in one place so the notebook is easy to adapt across projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Resolve project directory automatically ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Works in VS Code, JupyterLab, and classic Jupyter Notebook.\n",
    "_nb_path = globals().get(\"__vsc_ipynb_file__\")  # VS Code injects this\n",
    "if _nb_path:\n",
    "    PROJECT_DIR = Path(_nb_path).resolve().parent.parent  # notebooks/ ‚Üí project/\n",
    "else:\n",
    "    # Browser Jupyter sets CWD to the notebook's directory.\n",
    "    _cwd = Path.cwd()\n",
    "    PROJECT_DIR = next(\n",
    "        (p for p in [_cwd, *_cwd.parents]\n",
    "         if (p / \"data\").is_dir() and (p / \"notebooks\").is_dir()),\n",
    "        _cwd,\n",
    "    )\n",
    "\n",
    "# ‚îÄ‚îÄ Dataset ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Just set the filename ‚Äî the full path is resolved automatically.\n",
    "DATA_FILE = \"dataset.csv\"  # TODO: replace with your dataset filename\n",
    "DATA_PATH = PROJECT_DIR / \"data\" / DATA_FILE\n",
    "\n",
    "# TODO: Set the name of the target column in your dataset\n",
    "TARGET_COL = \"target\"\n",
    "\n",
    "# Task type ‚Äî drives conditional behaviour throughout the notebook:\n",
    "#   \"regression\"      ‚Üí histograms, scatter plots, correlation analysis\n",
    "#   \"classification\"  ‚Üí bar charts, box plots per class, class balance checks\n",
    "TASK = \"regression\"  # or \"classification\"\n",
    "\n",
    "print(f\"üìÅ Project dir: {PROJECT_DIR}\")\n",
    "print(f\"üìÑ Data path:   {DATA_PATH}  (exists: {DATA_PATH.exists()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279bf085",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ 3 ¬∑ Load Data\n",
    "\n",
    "Load the raw dataset and take a first look at its shape, types, and sample rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ade33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data quality summary card ---\n",
    "n_rows, n_cols = df.shape\n",
    "dtypes_breakdown = df.dtypes.value_counts().to_dict()\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = n_rows * n_cols\n",
    "missing_pct = (total_missing / total_cells * 100)\n",
    "n_duplicates = df.duplicated().sum()\n",
    "mem_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"  üìã DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Rows:            {n_rows:,}\")\n",
    "print(f\"  Columns:         {n_cols:,}\")\n",
    "print(f\"  Dtypes:          {dtypes_breakdown}\")\n",
    "print(f\"  Missing values:  {total_missing:,} ({missing_pct:.2f}%)\")\n",
    "print(f\"  Duplicate rows:  {n_duplicates:,}\")\n",
    "print(f\"  Memory usage:    {mem_mb:.2f} MB\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f019a",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ 4 ¬∑ Target Variable\n",
    "\n",
    "Separate features (`X`) and target (`y`) early.  \n",
    "By convention in ML, **`X`** denotes the feature matrix and **`y`** denotes the target vector ‚Äî this comes from the statistical notation $y = f(X) + \\varepsilon$ and is the standard used by scikit-learn, XGBoost, LightGBM, and virtually every ML library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67daf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "# By ML convention: X = features, y = target\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target: '{TARGET_COL}'  |  dtype: {y.dtype}  |  \"\n",
    "      f\"mean: {y.mean():.2f}  |  median: {y.median():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c68f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution ‚Äî conditional on TASK\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "if TASK == \"regression\":\n",
    "    y.hist(bins=30, edgecolor=\"black\", ax=axes[0])\n",
    "    axes[0].set_title(f\"{TARGET_COL} ‚Äî Distribution\")\n",
    "    axes[0].set_xlabel(TARGET_COL)\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "    axes[1].boxplot(y.dropna(), vert=True)\n",
    "    axes[1].set_title(f\"{TARGET_COL} ‚Äî Box Plot\")\n",
    "else:  # classification\n",
    "    counts = y.value_counts().sort_index()\n",
    "    counts.plot(kind=\"bar\", edgecolor=\"black\", ax=axes[0])\n",
    "    axes[0].set_title(f\"{TARGET_COL} ‚Äî Class Distribution\")\n",
    "    axes[0].set_xlabel(TARGET_COL)\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "    axes[0].tick_params(axis=\"x\", rotation=0)\n",
    "    # Class balance as percentage\n",
    "    pct = (counts / counts.sum() * 100).round(1)\n",
    "    pct.plot(kind=\"bar\", edgecolor=\"black\", ax=axes[1])\n",
    "    axes[1].set_title(f\"{TARGET_COL} ‚Äî Class Balance (%)\")\n",
    "    axes[1].set_ylabel(\"%\")\n",
    "    axes[1].tick_params(axis=\"x\", rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03557e",
   "metadata": {},
   "source": [
    "---\n",
    "## üï≥Ô∏è 5 ¬∑ Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39014c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = X.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "missing_pct = (missing / len(X) * 100).round(2)\n",
    "\n",
    "if missing.empty:\n",
    "    print(\"No missing values üéâ\")\n",
    "else:\n",
    "    print(pd.DataFrame({\"count\": missing, \"% of total\": missing_pct}).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1df28",
   "metadata": {},
   "source": [
    "---\n",
    "## üßπ 6 ¬∑ Data Cleaning\n",
    "\n",
    "Handle missing values, fix dtypes, remove duplicates, drop irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97662493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "n_dup = X.duplicated().sum()\n",
    "print(f\"Duplicates found: {n_dup}\")\n",
    "X = X.drop_duplicates()\n",
    "\n",
    "# TODO: Drop irrelevant columns (IDs, row numbers, etc.)\n",
    "# X = X.drop(columns=[\"id\"], errors=\"ignore\")\n",
    "\n",
    "# TODO: Handle missing values\n",
    "# X[\"col\"] = X[\"col\"].fillna(X[\"col\"].median())\n",
    "\n",
    "# TODO: Fix data types\n",
    "# X[\"col\"] = X[\"col\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e83ec",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä 7 ¬∑ Exploratory Data Analysis ‚Äî Univariate\n",
    "\n",
    "Distribution of individual features. Separate numeric from categorical using `select_dtypes` ‚Äî a standard pandas pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a35698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate feature types ‚Äî standard naming used across notebooks & train.py\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "print(f\"Numeric features:     {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric distributions ‚Äî 3 per row\n",
    "COLS_PER_ROW = 3\n",
    "n_num = len(numeric_features)\n",
    "n_rows = math.ceil(n_num / COLS_PER_ROW)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, COLS_PER_ROW, figsize=(5 * COLS_PER_ROW, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_features):\n",
    "    X[col].hist(bins=30, edgecolor=\"black\", ax=axes[i])\n",
    "    axes[i].set_title(col, fontsize=10)\n",
    "    axes[i].tick_params(labelsize=8)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(n_num, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Numeric Feature Distributions\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92626182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical value counts ‚Äî 2 per row (skip high-cardinality columns)\n",
    "HIGH_CARD_THRESHOLD = 20\n",
    "\n",
    "plot_cats = [c for c in categorical_features if X[c].nunique() <= HIGH_CARD_THRESHOLD]\n",
    "skipped = [c for c in categorical_features if X[c].nunique() > HIGH_CARD_THRESHOLD]\n",
    "if skipped:\n",
    "    print(f\"‚ö†Ô∏è  Skipping high-cardinality columns: {', '.join(skipped)}\")\n",
    "\n",
    "CAT_COLS_PER_ROW = 2\n",
    "n_cats = len(plot_cats)\n",
    "n_cat_rows = math.ceil(n_cats / CAT_COLS_PER_ROW)\n",
    "\n",
    "fig, axes = plt.subplots(n_cat_rows, CAT_COLS_PER_ROW,\n",
    "                         figsize=(7 * CAT_COLS_PER_ROW, 4 * n_cat_rows))\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for i, col in enumerate(plot_cats):\n",
    "    counts = X[col].value_counts().head(15)\n",
    "    counts.sort_values().plot(kind=\"barh\", ax=axes[i])\n",
    "    axes[i].set_title(col, fontsize=10)\n",
    "    axes[i].set_xlabel(\"Count\")\n",
    "    axes[i].tick_params(labelsize=8)\n",
    "\n",
    "for j in range(n_cats, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle(\"Categorical Feature Distributions\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b86fa",
   "metadata": {},
   "source": [
    "---\n",
    "## üîó 8 ¬∑ Exploratory Data Analysis ‚Äî Bivariate / Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc47f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix (numeric features only)\n",
    "corr = X[numeric_features].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=False, cmap=\"coolwarm\",\n",
    "            center=0, square=True, linewidths=0.5, ax=ax)\n",
    "ax.set_title(\"Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2aa30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlations with target\n",
    "target_corr = X[numeric_features].corrwith(y).sort_values(ascending=False)\n",
    "print(\"Top positive correlations with target:\")\n",
    "print(target_corr.head(10).to_string())\n",
    "print(\"\\nTop negative correlations with target:\")\n",
    "print(target_corr.tail(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a80b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target vs top numeric features ‚Äî grid layout\n",
    "top_n = 5\n",
    "top_features = target_corr.abs().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "\n",
    "BIV_COLS = 3\n",
    "biv_rows = math.ceil(len(top_features) / BIV_COLS)\n",
    "fig, axes = plt.subplots(biv_rows, BIV_COLS, figsize=(6 * BIV_COLS, 5 * biv_rows))\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for i, col in enumerate(top_features):\n",
    "    if TASK == \"regression\":\n",
    "        axes[i].scatter(X[col], y, alpha=0.3, edgecolors=\"k\", linewidths=0.3)\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel(TARGET_COL)\n",
    "        axes[i].set_title(f\"{col} vs {TARGET_COL}\")\n",
    "    else:  # classification\n",
    "        sns.stripplot(x=y, y=X[col], ax=axes[i], alpha=0.3, jitter=True)\n",
    "        axes[i].set_title(f\"{col} by {TARGET_COL} class\")\n",
    "\n",
    "for j in range(len(top_features), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle(f\"Top {top_n} Features vs {TARGET_COL}\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c26e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target vs categorical features ‚Äî mean target per category (2 per row)\n",
    "CAT_BIV_COLS = 2\n",
    "cat_biv_rows = math.ceil(len(plot_cats) / CAT_BIV_COLS)\n",
    "\n",
    "fig, axes = plt.subplots(cat_biv_rows, CAT_BIV_COLS,\n",
    "                         figsize=(7 * CAT_BIV_COLS, 4 * cat_biv_rows))\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for i, col in enumerate(plot_cats):\n",
    "    means = df.groupby(col)[TARGET_COL].mean().sort_values(ascending=True)\n",
    "    means.plot(kind=\"barh\", ax=axes[i])\n",
    "    axes[i].set_title(f\"Mean {TARGET_COL} by {col}\", fontsize=10)\n",
    "    axes[i].set_xlabel(f\"Mean {TARGET_COL}\")\n",
    "    axes[i].tick_params(labelsize=8)\n",
    "\n",
    "for j in range(len(plot_cats), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle(f\"Mean {TARGET_COL} by Category\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312cb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bd59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot ‚Äî top 5 numeric features most correlated with target\n",
    "pair_cols = top_features + [TARGET_COL]\n",
    "pair_df = df[pair_cols].dropna()\n",
    "\n",
    "g = sns.pairplot(pair_df, corner=True, plot_kws={\"alpha\": 0.3, \"s\": 10})\n",
    "g.figure.suptitle(\"Pairplot ‚Äî Top 5 Correlated Features\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1abfaf",
   "metadata": {},
   "source": [
    "---\n",
    "## üö® 9 ¬∑ Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1df4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR-based outlier summary\n",
    "def outlier_report(dataframe, cols):\n",
    "    records = []\n",
    "    for col in cols:\n",
    "        Q1 = dataframe[col].quantile(0.25)\n",
    "        Q3 = dataframe[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        n_out = ((dataframe[col] < lower) | (dataframe[col] > upper)).sum()\n",
    "        records.append({\"feature\": col, \"n_outliers\": n_out,\n",
    "                        \"% outliers\": round(n_out / len(dataframe) * 100, 2)})\n",
    "    return pd.DataFrame(records).sort_values(\"n_outliers\", ascending=False)\n",
    "\n",
    "outlier_report(X, numeric_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840cf295",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° 10 ¬∑ EDA Summary\n",
    "\n",
    "### Dataset Overview\n",
    "- **Rows:** *...*\n",
    "- **Columns:** *...*\n",
    "- **Task:** *regression / classification*\n",
    "\n",
    "### Data Quality Findings\n",
    "- *e.g. X columns have >50% missing values*\n",
    "- *e.g. N duplicate rows removed*\n",
    "- *e.g. columns A, B have inconsistent dtypes*\n",
    "\n",
    "### Target Variable Observations\n",
    "- *e.g. right-skewed distribution ‚Üí consider log transform*\n",
    "- *e.g. class imbalance: 90/10 split ‚Üí consider SMOTE or class weights*\n",
    "\n",
    "### Key Feature Insights\n",
    "- *e.g. Feature X has high cardinality (500+ unique values)*\n",
    "- *e.g. Feature Y shows clear separation between classes*\n",
    "\n",
    "### Correlations Worth Investigating\n",
    "- *e.g. Feature A and B are highly correlated (r=0.95) ‚Üí possible multicollinearity*\n",
    "- *e.g. Feature C has the strongest relationship with the target*\n",
    "\n",
    "### Recommended Preprocessing Steps\n",
    "- [ ] *e.g. Drop columns with >60% missing*\n",
    "- [ ] *e.g. Impute column X with median*\n",
    "- [ ] *e.g. Log-transform target*\n",
    "- [ ] *e.g. One-hot encode low-cardinality categoricals*\n",
    "- [ ] *e.g. Move final pipeline to `src/train.py`*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-projects (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
